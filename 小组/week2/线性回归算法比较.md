## 自定义实现与Sklearn的SGDRegressor比较

1. **初始化**：
   - Sklearn的SGDRegressor提供了更多的初始化选项，例如通过'init'参数可以选择不同的方法来初始化权重，而自定义实现将权重初始化为全零。

2. **正则化**：
   - 自定义实现包含了L2正则化项，通过在梯度计算中添加对权重的惩罚来控制模型复杂度，防止过拟合。而Sklearn的SGDRegressor除了L2正则化外，还支持L1正则化和弹性网络正则化。

3. **学习率调度**：
   - Sklearn的SGDRegressor可以使用不同的学习率调度策略，例如常数、最优或自适应学习率。而自定义实现中的学习率是固定的，在整个训练过程中不会改变。

4. **提前停止**：
   - Sklearn的SGDRegressor支持提前停止策略，可以根据验证集上的性能来决定何时停止训练，以防止过拟合。而自定义实现没有实现提前停止功能。

5. **性能指标**：
   - Sklearn的SGDRegressor提供了更多的性能评估指标，例如均方误差（Mean Squared Error）、平均绝对误差（Mean Absolute Error）等，而自定义实现只提供了R^2分数作为评估指标。

6. **向量化**：
   - Sklearn的实现通常经过高度的向量化处理以提高效率，例如通过矩阵运算来加速计算，而自定义实现中的梯度计算和更新权重的操作可能不够高效。

7. **可扩展性**：
   - Sklearn的SGDRegressor针对大规模数据集进行了优化，可以处理高维数据和大量样本，而自定义实现可能在处理大规模数据时效率较低。

### 手搓的SGDRegressor：
```python
# 线性回归之梯度下降法
class SGDRegressor:
    def __init__(self, learning_rate=0.1, n_iterations=2000, regularization_strength=0.01):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.regularization_strength = regularization_strength
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        # 初始化权重和偏差
        self.weights = np.zeros(X.shape[1])
        self.bias = 0

        for _ in range(self.n_iterations):
            # 计算预测值
            y_pred = np.dot(X, self.weights) + self.bias

            # 计算梯度
            grad_weights = -2 * np.dot(X.T, (y - y_pred)) / X.shape[0] + 2 * self.regularization_strength * self.weights
            grad_bias = -2 * np.sum(y - y_pred) / X.shape[0]

            # 更新权重和偏差
            self.weights -= self.learning_rate * grad_weights
            self.bias -= self.learning_rate * grad_bias

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

    def score(self, X, y):
        y_pred = self.predict(X)
        ssr = np.sum((y_pred - np.mean(y))**2)
        sse = np.sum((y - y_pred)**2)
        r_squared = 1 - (sse / ssr)
        return r_squared
```

